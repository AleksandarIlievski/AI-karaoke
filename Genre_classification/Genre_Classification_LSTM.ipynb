{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2BlrFG-LSjmO",
        "we81ZRwxSw_n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Genre Classification based on Song Lyrics**"
      ],
      "metadata": {
        "id": "2BlrFG-LSjmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download the Dataset\n",
        "The datasets used for training and validation were downloaded from Kaggle. The following section downloads the datasets and saves them in the current colab repository. In order to do so, you need a Kaggle account and [download the Kaggle API token](https://medium.com/unpackai/how-to-use-kaggle-datasets-in-google-colab-f9b2e4b5767c). We uploaded the used datasets in our github repository, so you could also skip this part and access them from our github repository instead."
      ],
      "metadata": {
        "id": "mU3g8teudSzw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFiH9pCBSXnU"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload JSON file with account info from kaggle\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "sBvNFP6xSnak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make directory named kaggle and copy kaggle.json file there\n",
        "! mkdir ~/.kaggle\n",
        "# Choose the kaggle.json file that you downloaded\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "# Download dataset from kaggle\n",
        "! kaggle datasets download neisse/scrapped-lyrics-from-6-genres\n",
        "! unzip scrapped-lyrics-from-6-genres\n",
        "! rm -r scrapped-lyrics-from-6-genres.zip"
      ],
      "metadata": {
        "id": "GNQD6qg2Spfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing\n",
        "The training and validation dataset consists of two csv files. We first merge the two csv files. Then, we take a look at the \"Genres\" column. Every song has multiple genres asigned. We will first split the string that contains the multiple genres and save them in different columns (\"Genre1\", \"Genre2\"...). For the classification we will only use the \"Genre1\" column and select 7 genres that have the highest sample sizes. After looking closer at the dataset, we notice an imbalance in sample size between the different genres. In order to make the dataset more balanced we will swap some samples to different genres, e.g. \"Rap\" was added to \"Hip Hop\" or \"Black Music\" was added to \"R&B\". The 7 genres used for classification are: \"Rock\",\"Hip Hop\",\"Pop\",\"Indie\",\"Heavy Metal\",\"R&B\", \"Country\". At the end, the lyrics in the dataset are cleaned (e.g. remove special characters), the labels are encoded into integers and a train-test split was created."
      ],
      "metadata": {
        "id": "we81ZRwxSw_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "lyrics_data = pd.read_csv(\"/content/lyrics-data.csv\")\n",
        "artists_data = pd.read_csv(\"/content/artists-data.csv\")\n",
        "\n",
        "lyrics_data = lyrics_data.rename({\"ALink\":\"Link\"}, axis=\"columns\")\n",
        "data = lyrics_data.merge(artists_data, on='Link', how='left')\n",
        "data = data.drop([\"Link\", \"SName\", \"SLink\", \"Songs\", \"Popularity\"], axis=1)\n",
        "data = data.loc[data[\"language\"] == \"en\"]\n",
        "data[['Genre1', 'Genre2', 'Genre3', 'Genre4']] = data['Genres'].str.split(';', -1, expand=True)\n",
        "data = data[data['Genre1'].notna()]\n",
        "data = data.drop(\"Genres\", axis=1)\n",
        "data = data.reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "other_genres = []\n",
        "for genre in data['Genre1'].unique():\n",
        "  freq = len(data[data[\"Genre1\"] == genre])\n",
        "  if freq <1200:\n",
        "    other_genres.append(genre)\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "  if (row[\"Genre1\"] == \"Pop/Rock\") & (row[\"Genre2\"] == \" Pop\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Pop\"\n",
        "  \n",
        "  if (row[\"Genre1\"] == \"Rap\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Hip Hop\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Rock Alternativo\") & (row[\"Genre2\"] == \" Indie\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Indie\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Black Music\") & (row[\"Genre2\"] == \" R&B\"):\n",
        "    data.loc[index, \"Genre1\"] = \"R&B\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Black Music\") & (row[\"Genre2\"] == \" Soul Music\"):\n",
        "    data.loc[index, \"Genre1\"] = \"R&B\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Soul Music\") & (row[\"Genre2\"] == \" R&B\"):\n",
        "    data.loc[index, \"Genre1\"] = \"R&B\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Soul Music\") & (row[\"Genre2\"] == \" Pop\"):\n",
        "    data.loc[index, \"Genre1\"] = \"R&B\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Dance\") & (row[\"Genre2\"] == \" Pop\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Pop\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Folk\") & (row[\"Genre2\"] == \" Indie\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Indie\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Folk\") & (row[\"Genre3\"] == \" Indie\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Indie\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Romântico\") & (row[\"Genre3\"] == \" R&B\"):\n",
        "    data.loc[index, \"Genre1\"] = \"R&B\"\n",
        "  \n",
        "  if (row[\"Genre1\"] == \"Pop\") & (row[\"Genre2\"] == \" R&B\"):\n",
        "    data.loc[index, \"Genre1\"] = \"R&B\"\n",
        "\n",
        "  if (row[\"Genre1\"] == \"Rock\") & (row[\"Genre2\"] == \" Country\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Country\" \n",
        "  \n",
        "  if (row[\"Genre1\"] == \"Trilha Sonora\") & (row[\"Genre2\"] == \" Country\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Country\" \n",
        "\n",
        "  if (row[\"Genre1\"] == \"Folk\") & (row[\"Genre2\"] == \" Country\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Country\" \n",
        "\n",
        "  if row[\"Genre1\"] in other_genres:\n",
        "    data.loc[index, \"Genre1\"] = \"Other\" \n",
        "  \n",
        "  ###New changes:###\n",
        "  if (row[\"Genre1\"] == \"Rock\") & (row[\"Genre2\"] == \" Pop\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Pop\"  \n",
        "\n",
        "  if (row[\"Artist\"] == \"Wilco\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Country\" \n",
        "\n",
        "  if (row[\"Genre1\"] == \"Rock\") & (row[\"Genre2\"] == \" Indie\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Indie\"  \n",
        "\n",
        "  if (row[\"Artist\"] == \"Steve Earle\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Country\" \n",
        "\n",
        "  if (row[\"Genre1\"] == \"Rock\") & (row[\"Genre2\"] == \" Heavy Metal\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Heavy Metal\"  \n",
        "\n",
        "  if (row[\"Genre1\"] == \"Pop\") & (row[\"Genre3\"] == \" R&B\"):\n",
        "    data.loc[index, \"Genre1\"] = \"Pop\"  \n",
        "  \n",
        "  if (row[\"Artist\"] in ['Ciara', 'Jeremih', 'Kehlani','Ashanti', 'Jhené Aiko', 'Ray J', 'Omarion', 'Mindless Behavior']):\n",
        "        data.loc[index, \"Genre1\"] = \"R&B\"  \n",
        "\n",
        "genre_list = [\"Rock\",\"Hip Hop\",\"Pop\",\"Indie\",\"Heavy Metal\",\"R&B\", \"Country\"]\n",
        "data = data[data[\"Genre1\"].isin(genre_list)]"
      ],
      "metadata": {
        "id": "Nk5yJJShStW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_list = []\n",
        "for genre in data['Genre1'].unique():\n",
        "  freq = len(data[data[\"Genre1\"] == genre])\n",
        "  freq_list.append((freq, genre))\n",
        "\n",
        "print(sorted(freq_list, key = lambda x: x[0], reverse=True))"
      ],
      "metadata": {
        "id": "9xLT2wMilDDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_labels = {}\n",
        "i = 0\n",
        "for label in data[\"Genre1\"].unique():\n",
        "  decode_labels[i] = label\n",
        "  i += 1\n",
        "encode_labels = {v: k for k, v in decode_labels.items()}\n",
        "\n",
        "import re \n",
        "\n",
        "def clean_lyrics(lyrics):\n",
        "    lyrics = re.sub(r\"[^A-Za-z0-9']+\", \" \", lyrics, flags=re.MULTILINE)\n",
        "    lyrics = re.sub(r\"(?<=[a-zA-Z0-9]) (?=['])|(?<=[']) (?=[a-zA-Z0-9])\", \"\", lyrics, flags=re.MULTILINE)\n",
        "    lyrics = re.sub('\\s+',' ',lyrics)\n",
        "    return lyrics.lower().lstrip()\n",
        "\n",
        "for index, row in data.iterrows(): #encode labels in dataset \n",
        "  label = encode_labels[row[\"Genre1\"]]\n",
        "  data.loc[index,'Label'] = label\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.05)"
      ],
      "metadata": {
        "id": "GR6Q6uwzeI9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Build Vocab and Dataloader\n"
      ],
      "metadata": {
        "id": "GUaUmwZAhMU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)\n",
        "\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "  def __init__(self, dataset: pd.DataFrame):\n",
        "      self.lyrics = dataset['Lyric'].tolist()\n",
        "      self.labels = dataset[\"Label\"].tolist()\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.lyrics)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "      feat = clean_lyrics(self.lyrics[index])\n",
        "      label = int(self.labels[index])\n",
        "      \n",
        "      return (label, feat)\n",
        "  \n",
        "  def collate_fn(batch):\n",
        "      label_list = [torch.tensor(label_pipeline(b[0])) for b in batch]\n",
        "      feat_list = [torch.tensor(text_pipeline(b[1])) for b in batch]\n",
        "\n",
        "      feats = torch.nn.utils.rnn.pad_sequence(feat_list, batch_first = True)\n",
        "      labels = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "      return feats.to(device), labels.to(device)\n",
        "\n",
        "    \n",
        "train_iter = LyricsDataset(dataset=train_data)\n",
        "test_iter = LyricsDataset(dataset=test_data)\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\",\"<unk>\"], min_freq = 500)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=True, collate_fn=LyricsDataset.collate_fn)\n",
        "test_loader = DataLoader(test_iter, batch_size=8, shuffle=False, collate_fn=LyricsDataset.collate_fn)"
      ],
      "metadata": {
        "id": "DmaNthSFCjTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train Model\n",
        "An LSTM model with an embedding and linear layer is trained. Since the LSTM model tends to overfit, we added a dropout layer in the model architecture. For training, we use Adam optimizer. To further avoid overfitting, [we also added weight decay](https://medium.com/analytics-vidhya/deep-learning-basics-weight-decay-3c68eb4344e9) to it in order to keep the weights as small as possible, preventing the weights to grow out of control, and thus avoid exploding gradient. Furthermore, we chose a dynamic learning rate, that reduces by 0.5 when the validation loss plateaus. We also added early stopping when the model plateaus and save the best model checkpoints to avoid an overfitted model. After the training, the state_dict and vocabulary of the model are saved."
      ],
      "metadata": {
        "id": "dLLOSCFYVTDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "embed_len = 50\n",
        "hidden_dim = 75\n",
        "n_layers=1\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, num_class)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim), torch.randn(n_layers, len(X_batch), hidden_dim)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden.to(device), carry.to(device)))\n",
        "        output = self.dropout(output)\n",
        "        return self.linear(output[:,-1])"
      ],
      "metadata": {
        "id": "J3VZlIT6YPS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_classifier = LSTMClassifier().to(device)\n",
        "\n",
        "lstm_classifier"
      ],
      "metadata": {
        "id": "84XmVWJUY9D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in lstm_classifier.children():\n",
        "    print(\"Layer : {}\".format(layer))\n",
        "    print(\"Parameters : \")\n",
        "    for param in layer.parameters():\n",
        "        print(param.shape)\n",
        "    print()"
      ],
      "metadata": {
        "id": "8fY4OceNY9ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's \n",
        "    validation loss is less than the previous least less, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        \n",
        "    def __call__(\n",
        "        self, current_valid_loss, \n",
        "        epoch, model, optimizer, criterion\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
        "            print(f\"\\nSaving best model for epoch: {epoch}\\n\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, '/content/best_model.pth')\n",
        "\n",
        "save_best_model = SaveBestModel()\n",
        "\n",
        "class EarlyStopping():\n",
        "    \"\"\"\n",
        "    Early stopping to stop the training when the loss does not improve after\n",
        "    certain epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=8, min_delta=0):\n",
        "        \"\"\"\n",
        "        :param patience: how many epochs to wait before stopping when loss is\n",
        "               not improving\n",
        "        :param min_delta: minimum difference between new loss and old loss for\n",
        "               new loss to be considered as an improvement\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            # reset counter if validation loss improves\n",
        "            self.counter = 0\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                print('INFO: Early stopping')\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopping = EarlyStopping()"
      ],
      "metadata": {
        "id": "XFBdpXhouiuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader, epoch):\n",
        "    with torch.no_grad():\n",
        "        Y_shuffled, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_shuffled.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_shuffled = torch.cat(Y_shuffled)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        early_stopping(torch.tensor(losses).mean())\n",
        "\n",
        "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.cpu().detach().numpy(), Y_preds.cpu().detach().numpy())))\n",
        "        save_best_model(torch.tensor(losses).mean(), epoch, model, optimizer, loss_fn) # save best checkpoint added\n",
        "        print('-'*50)\n",
        "        scheduler.step(torch.tensor(losses).mean()) # scheduler added\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = model(X) ## Make Predictions\n",
        "\n",
        "            loss = loss_fn(Y_preds, Y) ## Calculate Loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad() ## Clear previously calculated gradients\n",
        "            loss.backward() ## Calculates Gradients\n",
        "            optimizer.step() ## Update network weights.\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        CalcValLossAndAccuracy(model, loss_fn, val_loader, i)\n",
        "        if early_stopping.early_stop:\n",
        "          break"
      ],
      "metadata": {
        "id": "MFhH3eIpZrMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "epochs = 100\n",
        "learning_rate = 0.001#1e-3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "lstm_classifier = LSTMClassifier().to(device)\n",
        "optimizer = Adam(lstm_classifier.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
        "                optimizer,\n",
        "                mode='min',\n",
        "                patience=2,\n",
        "                factor=0.5,\n",
        "                min_lr=1e-6,\n",
        "                verbose=True\n",
        "            ) # scheduler added\n",
        "\n",
        "TrainModel(lstm_classifier, loss_fn, optimizer, dataloader, test_loader, epochs)"
      ],
      "metadata": {
        "id": "YcIl18ZLZtuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(lstm_classifier.state_dict(), \"lstm_model.pth\")\n",
        "torch.save({'epoch': epochs,\n",
        "            'model_state_dict': lstm_classifier.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss_fn,}, 'final_model.pth')\n",
        "torch.save(vocab, 'vocab.pth')"
      ],
      "metadata": {
        "id": "2gukU6XojB7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Evaluation"
      ],
      "metadata": {
        "id": "1JOSeJ75maBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4Pj6Hgx1l9Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MakePredictions(model, loader):\n",
        "      Y_shuffled, Y_preds = [], []\n",
        "      with torch.no_grad():\n",
        "        for X, Y in loader:\n",
        "            preds = model(X)\n",
        "            Y_preds.append(preds)\n",
        "            Y_shuffled.append(Y)\n",
        "        gc.collect()\n",
        "        Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "\n",
        "      return Y_shuffled.cpu().detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy()\n",
        "\n",
        "Y_actual, Y_preds = MakePredictions(lstm_classifier, test_loader)\n"
      ],
      "metadata": {
        "id": "IXurqTvBgjqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_classes = []\n",
        "for index, genre in decode_labels.items():\n",
        "  target_classes.append(genre)\n",
        "\n",
        "print(target_classes)"
      ],
      "metadata": {
        "id": "JRGhlQDirm54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ],
      "metadata": {
        "id": "8Feun8Sugm5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-plot"
      ],
      "metadata": {
        "id": "niR7oukyguwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_actual], [target_classes[i] for i in Y_preds],\n",
        "                                    normalize=True,\n",
        "                                    title=\"Confusion Matrix\",\n",
        "                                    cmap=\"Purples\",\n",
        "                                    hide_zeros=True,\n",
        "                                    figsize=(5,5)\n",
        "                                    );\n",
        "plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "YNXKBpXhgpdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(\"confusion_matrix\")"
      ],
      "metadata": {
        "id": "nx5E9x9GqTnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}