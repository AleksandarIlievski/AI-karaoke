{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "ANNpAFxea6Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d9QuJj4ba_SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_PATH = '/content/drive/MyDrive/ASR-Praktikum/vocab.pth'\n",
        "MODEL_PATH = '/content/drive/MyDrive/ASR-Praktikum/final_model.pth'"
      ],
      "metadata": {
        "id": "-iMoIJ3WbAfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvv9-YPh7peb"
      },
      "source": [
        "# Karaok AI\n",
        "\n",
        "In this notebook you can run forced alignment for youtube music videos. The results are displayed on a website using dash. The website provides an embedding of the youtube video and the highlighted transcript based on the timestamp of the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYL7qtzLC77u"
      },
      "outputs": [],
      "source": [
        "!pip install pydub youtube-dl spleeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPg1ZJXI_V-2"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "import shutil\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from io import BytesIO\n",
        "\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMG11VA8CWKF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_spleeter = True"
      ],
      "metadata": {
        "id": "in2Iobp_NjBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Model\n",
        "\n",
        "We are using Pytorchs' WAV2VEC2_ASR_BASE_960H pre-trained on 960 hours of unlabeled audio from LibriSpeech dataset [Panayotov et al., 2015] (the combination of “train-clean-100”, “train-clean-360”, and “train-other-500”), and fine-tuned for ASR on the same audio with the corresponding transcripts."
      ],
      "metadata": {
        "id": "Uumg1V27bKPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvJUpvVsYK7M"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
        "labels = bundle.get_labels()\n",
        "model = bundle.get_model().to(device)\n",
        "dictionary = {c: i for i, c in enumerate(labels)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download YouTube Video"
      ],
      "metadata": {
        "id": "z_1fEtUSbTQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_spleeter:\n",
        "  from spleeter.separator import Separator\n",
        "  # Initialize the separator\n",
        "  separator = Separator('spleeter:2stems')"
      ],
      "metadata": {
        "id": "uKRA6nb0NbqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuiWyfDDzI3h"
      },
      "outputs": [],
      "source": [
        "def get_wave(aud):\n",
        "  aud = aud.set_channels(1)\n",
        "  aud = aud.get_array_of_samples()\n",
        "  wave = torch.tensor(aud, dtype = torch.float)\n",
        "  wave = torch.reshape(wave, (1,wave.shape[0]))\n",
        "\n",
        "  return wave"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wav_sr_from_yt_video_id(video_id):\n",
        "    # Download the video using youtube-dl\n",
        "    os.system(\"youtube-dl --extract-audio --audio-format wav --audio-quality 0 -o '%(id)s.%(ext)s' https://youtu.be/{}\".format(video_id))\n",
        "\n",
        "    file_path = \"{}.wav\".format(video_id)\n",
        "    audio_path = file_path\n",
        "\n",
        "    if use_spleeter:\n",
        "      separator.separate_to_file(\"/content/{}.wav\".format(video_id), \"/content/\")\n",
        "      audio_path = \"/content/{}/vocals.wav\".format(video_id)\n",
        "\n",
        "    # Load the audio file using pydub\n",
        "    audio = AudioSegment.from_file(audio_path, format=\"wav\")\n",
        "\n",
        "    waveform = get_wave(audio)\n",
        "    sr = audio.frame_rate\n",
        "\n",
        "    # Delete file\n",
        "    if os.path.isfile(file_path):\n",
        "        os.remove(file_path)\n",
        "    else:\n",
        "        print(\"{} does not exist.\".format(file_path))\n",
        "\n",
        "    try:\n",
        "        shutil.rmtree(\"/content/{}\".format(video_id))\n",
        "    except OSError as e:\n",
        "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "        \n",
        "    # Resample\n",
        "    if sr != bundle.sample_rate:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, bundle.sample_rate)\n",
        "\n",
        "    return waveform, sr"
      ],
      "metadata": {
        "id": "M4OOyZbzKA7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6U_cJvYBkL4"
      },
      "outputs": [],
      "source": [
        "def clean_lyrics(lyrics):\n",
        "    lyrics = re.sub(r\"\\[.*?\\]\", \"\", lyrics, flags=re.MULTILINE)\n",
        "    lyrics = re.sub(r\"’\", \"'\", lyrics)\n",
        "    lyrics = re.sub(r\"[^a-zA-Z'’|-]|\\s\", \"|\", lyrics)\n",
        "    return lyrics.upper()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forced Alignment\n",
        "Based on Pytorchs' FORCED ALIGNMENT WITH WAV2VEC2."
      ],
      "metadata": {
        "id": "e0hY_Oz5bYx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Emission of model"
      ],
      "metadata": {
        "id": "QHO_DoWCbejt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru8UVKPBHGhv"
      },
      "outputs": [],
      "source": [
        "def calculate_emission(waveform):\n",
        "    torch.cuda.empty_cache()    \n",
        "    \n",
        "    length = waveform.shape[1]\n",
        "    chunks = []\n",
        "    amount_chunks = 10\n",
        "    chunks_length = length//amount_chunks\n",
        "    for i in range(amount_chunks):\n",
        "        with torch.inference_mode():\n",
        "            emissions, _ = model(waveform[:, i * chunks_length: min(length, (i + 1) * chunks_length)].to(device))\n",
        "            emissions = torch.log_softmax(emissions, dim=-1)\n",
        "            chunks.append(emissions)\n",
        "\n",
        "    return torch.cat(chunks, dim=1)[0].cpu().detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate alignment probability (trellis)"
      ],
      "metadata": {
        "id": "lhv2q6lqbiaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iv9CYtlFOSA"
      },
      "outputs": [],
      "source": [
        "def get_tokens(transcript):\n",
        "    return [dictionary[c] for c in transcript]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rznzh7LnnHri"
      },
      "outputs": [],
      "source": [
        "def get_trellis(emission, tokens, blank_id=0):\n",
        "    num_frame = emission.size(0)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Trellis has extra diemsions for both time axis and tokens.\n",
        "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
        "    # The extra dim for time axis is for simplification of the code.\n",
        "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
        "    trellis[0, 0] = 0\n",
        "    trellis[1:, 0] = torch.cumsum(emission[:, 0], 0)\n",
        "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
        "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
        "\n",
        "    for t in range(num_frame):\n",
        "        trellis[t + 1, 1:] = torch.maximum(\n",
        "            # Score for staying at the same token\n",
        "            trellis[t, 1:] + emission[t, blank_id],\n",
        "            # Score for changing to the next token\n",
        "            trellis[t, :-1] + emission[t, tokens],\n",
        "        )\n",
        "    return trellis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y3Mf-_qnHrk"
      },
      "source": [
        "## Find the most likely path (backtracking)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wMrjfxdnHrl"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Point:\n",
        "    token_index: int\n",
        "    time_index: int\n",
        "    score: float\n",
        "\n",
        "\n",
        "def backtrack(trellis, emission, tokens, blank_id=0):\n",
        "    # Note:\n",
        "    # j and t are indices for trellis, which has extra dimensions\n",
        "    # for time and tokens at the beginning.\n",
        "    # When referring to time frame index `T` in trellis,\n",
        "    # the corresponding index in emission is `T-1`.\n",
        "    # Similarly, when referring to token index `J` in trellis,\n",
        "    # the corresponding index in transcript is `J-1`.\n",
        "    j = trellis.size(1) - 1\n",
        "    t_start = torch.argmax(trellis[:, j]).item()\n",
        "\n",
        "    path = []\n",
        "    for t in range(t_start, 0, -1):\n",
        "        # 1. Figure out if the current position was stay or change\n",
        "        # Note (again):\n",
        "        # `emission[J-1]` is the emission at time frame `J` of trellis dimension.\n",
        "        # Score for token staying the same from time frame J-1 to T.\n",
        "        stayed = trellis[t - 1, j] + emission[t - 1, blank_id]\n",
        "        # Score for token changing from C-1 at T-1 to J at T.\n",
        "        changed = trellis[t - 1, j - 1] + emission[t - 1, tokens[j - 1]]\n",
        "\n",
        "        # 2. Store the path with frame-wise probability.\n",
        "        prob = emission[t - 1, tokens[j - 1] if changed > stayed else 0].exp().item()\n",
        "        # Return token index and time index in non-trellis coordinate.\n",
        "        path.append(Point(j - 1, t - 1, prob))\n",
        "\n",
        "        # 3. Update the token\n",
        "        if changed > stayed:\n",
        "            j -= 1\n",
        "            if j == 0:\n",
        "                break\n",
        "    else:\n",
        "        raise ValueError(\"Failed to align\")\n",
        "    return path[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTmzO9WBnHrn"
      },
      "outputs": [],
      "source": [
        "# Merge the labels\n",
        "@dataclass\n",
        "class Segment:\n",
        "    label: str\n",
        "    start: int\n",
        "    end: int\n",
        "    score: float\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start}, {self.end})\"\n",
        "\n",
        "    @property\n",
        "    def length(self):\n",
        "        return self.end - self.start\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.label, self.start, self.end, self.score))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if not isinstance(other, Segment):\n",
        "            return False\n",
        "        return (self.label, self.start, self.end, self.score) == (other.label, other.start, other.end, other.score)\n",
        "\n",
        "\n",
        "\n",
        "def merge_repeats(path, transcript):\n",
        "    i1, i2 = 0, 0\n",
        "    segments = []\n",
        "    while i1 < len(path):\n",
        "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
        "            i2 += 1\n",
        "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
        "        segments.append(\n",
        "            Segment(\n",
        "                transcript[path[i1].token_index],\n",
        "                path[i1].time_index,\n",
        "                path[i2 - 1].time_index + 1,\n",
        "                score,\n",
        "            )\n",
        "        )\n",
        "        i1 = i2\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wMoDWl9nHrs"
      },
      "outputs": [],
      "source": [
        "# Merge words\n",
        "def merge_words(segments, ratio, sr, separator=\"|\"):\n",
        "    words = []\n",
        "    i1, i2 = 0, 0\n",
        "    while i1 < len(segments):\n",
        "        if i2 >= len(segments) or segments[i2].label == separator:\n",
        "            if i1 != i2:\n",
        "                segs = segments[i1:i2]\n",
        "                word = \"\".join([seg.label for seg in segs])\n",
        "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
        "\n",
        "                x0 = int(ratio * segments[i1].start)\n",
        "                x1 = int(ratio * segments[i2 - 1].end)\n",
        "                start = x0 / sr\n",
        "                end = x1 / sr\n",
        "\n",
        "                words.append(Segment(word, start, end, score))\n",
        "            i1 = i2 + 1\n",
        "            i2 = i1\n",
        "        else:\n",
        "            i2 += 1\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abt3GSLvnHrt"
      },
      "source": [
        "## Execute forced alignment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99C0VJxLEzO0"
      },
      "outputs": [],
      "source": [
        "def execute(audio, transcript):\n",
        "    transcript = clean_lyrics(transcript)\n",
        "    emission = calculate_emission(audio)\n",
        "    tokens = get_tokens(transcript)\n",
        "    trellis = get_trellis(emission, tokens)\n",
        "    path = backtrack(trellis, emission, tokens)\n",
        "    segments = merge_repeats(path, transcript)\n",
        "\n",
        "    ratio = audio.size(1) / (trellis.size(0) - 1)\n",
        "\n",
        "    word_segments = merge_words(segments, ratio=ratio, sr = bundle.sample_rate)\n",
        "    return emission, tokens, trellis, path, segments, word_segments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_with_id(video_id, transcript):\n",
        "    waveform, sr = get_wav_sr_from_yt_video_id(video_id)\n",
        "    return execute(waveform, transcript)"
      ],
      "metadata": {
        "id": "DgR_HR6C5CPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genre Prediction"
      ],
      "metadata": {
        "id": "3XOHgZtck2AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "embed_len = 50\n",
        "hidden_dim = 75\n",
        "n_layers=1\n",
        "num_class = 7\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, num_class)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        hidden, carry = torch.randn(n_layers, len(X_batch), hidden_dim), torch.randn(n_layers, len(X_batch), hidden_dim)\n",
        "        output, (hidden, carry) = self.lstm(embeddings, (hidden.to(device), carry.to(device)))\n",
        "        output = self.dropout(output)\n",
        "        return self.linear(output[:,-1])"
      ],
      "metadata": {
        "id": "9CTXtlpqV4um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab = torch.load(VOCAB_PATH)\n",
        "lstm_classifier = LSTMClassifier().to(device)\n",
        "optimizer = Adam(lstm_classifier.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "lstm_classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "lstm_classifier.eval()\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "import regex as re\n",
        "def clean_lyrics_2(lyrics):\n",
        "    lyrics = re.sub(r\"[^A-Za-z0-9']+\", \" \", lyrics, flags=re.MULTILINE)\n",
        "    lyrics = re.sub(r\"(?<=[a-zA-Z0-9]) (?=['])|(?<=[']) (?=[a-zA-Z0-9])\", \"\", lyrics, flags=re.MULTILINE)\n",
        "    lyrics = re.sub('\\s+',' ',lyrics)\n",
        "    return lyrics.lower().lstrip()\n",
        "\n",
        "decode_labels = {0:\"Pop\",1:\"R&B\",2:\"Hip Hop\",3:\"Rock\",4:\"Indie\",5:\"Country\",6:\"Heavy Metal\"}"
      ],
      "metadata": {
        "id": "C2dxy-XcWEVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_genre(lyrics):\n",
        "  pred = lstm_classifier(torch.tensor(text_pipeline(clean_lyrics_2(lyrics))).unsqueeze(0).to(device))\n",
        "  label = decode_labels[F.softmax(pred, dim=-1).argmax(dim=-1).item()]\n",
        "  return label"
      ],
      "metadata": {
        "id": "OkplshtIvFNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5r98YHbkoCD"
      },
      "source": [
        "# Website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oczo-aQF1y5t"
      },
      "outputs": [],
      "source": [
        "!pip install jupyter-dash\n",
        "!pip install dash-player"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tAGLtJW22L-"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmqgvp2x21n-"
      },
      "outputs": [],
      "source": [
        "def extract_video_id(link):\n",
        "    regExp = re.compile(r'^.*(youtu.be\\/|v\\/|u\\/\\w\\/|embed\\/|watch\\?v=|&v=)([^#&?]*).*')\n",
        "    match_id = regExp.match(link)\n",
        "    if match_id:\n",
        "        video_id = match_id.group(2)\n",
        "        if len(video_id) == 11:\n",
        "            return video_id \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysjNSJab2DeU"
      },
      "outputs": [],
      "source": [
        "from jupyter_dash import JupyterDash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output, State\n",
        "import dash_player"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "xaNdYyaRzT7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCiI_-3o3Ag1"
      },
      "outputs": [],
      "source": [
        "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
        "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAPZVSK6FfyX"
      },
      "outputs": [],
      "source": [
        "app.layout = html.Div([\n",
        "    html.H1(\"AI Karaoke\"),\n",
        "    # Yt link\n",
        "    html.Div([\n",
        "        dcc.Input(id=\"input_yt\", placeholder=\"Input Youtube Link\", style={'width': '600px', 'margin-right': '5px'}),\n",
        "        html.Button('Submit', id='btn_submit', n_clicks=0, style={'margin': '0px 5px'}),\n",
        "        html.Button('Reset', id='btn_reset', n_clicks=0, style={'margin': '0px 5px'}),\n",
        "        dcc.Loading(\n",
        "            id=\"loading-1\",\n",
        "            children=html.Div(id=\"loading-output\", style={'display': 'none'})\n",
        "        ),\n",
        "        html.Div(id=\"initial_message\", children=\"Enter a YouTube link and transcript and press submit to load video\", style={'display': 'block'}),\n",
        "        html.Div(id=\"invalid_link_div\", children=\"Invalid YouTube link\", style={'color': 'red', 'display': 'none'}),\n",
        "        html.Div(id=\"no_transcript_div\", children=\"Please enter a transcript\", style={'color': 'red', 'display': 'none'}),\n",
        "    ], style={'margin-bottom': '5px'}),\n",
        "\n",
        "    # video and transcript\n",
        "    html.Div([\n",
        "        dash_player.DashPlayer(id=\"player\", url=\"\", controls=True, width=\"70%\", height=\"80%\", style={'display': 'inline-block', 'margin-right': '10px'}),\n",
        "        # TODO change input as you need\n",
        "        dcc.Textarea(id='input_transcript', value=\"\", placeholder=\"Input transcript\", style={'width': '29%', 'height': '80%', 'display': 'inline-block'}),\n",
        "        html.Div(id='output_transcript', children=[\n",
        "            html.Div(id='pre_transcript', style={'display': 'inline'}), \n",
        "            html.Span(id='pre_highlight_word', style={'display': 'inline', 'background-color': '#ffffb3'}), \n",
        "            html.Span(id='highlight_word', style={'display': 'inline', 'background-color': 'orange'}), \n",
        "            html.Span(id='post_highlight_word', style={'display': 'inline', 'background-color': '#ffff99'}), \n",
        "            html.Div(id='post_transcript', style={'display': 'inline'})\n",
        "            ], style={'width': '29%', 'height': '80%', 'display': 'none', 'overflow-y': 'auto'})\n",
        "    ], style={'height': '100vh', 'width': '100%', 'display': 'flex'}),\n",
        "\n",
        "    html.Div([\n",
        "        html.Div(id=\"genre_div\", children=\"Predicted Genre:\", style={'display': 'inline-block', 'margin-right': '5px'}),\n",
        "        html.Div(id=\"genre_class\", children=\"\", style={'display': 'inline-block', 'font-weight': 'bold'}),\n",
        "    ]),\n",
        "\n",
        "    # current video timestamp, only for debugging\n",
        "    html.Div(id=\"div_current_time\", style={\"margin\": \"10px 0px\"}),\n",
        "    dcc.Interval(id='interval', interval=2, n_intervals=0),\n",
        "    dcc.Store(id='clientside-store-data')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbTNd7q-INrw"
      },
      "outputs": [],
      "source": [
        "submit_clicks = 0\n",
        "reset_clicks = 0\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    [Output('player', 'url'),\n",
        "    Output('initial_message', 'style'),\n",
        "    Output('invalid_link_div', 'style'),\n",
        "    Output('no_transcript_div', 'style'),\n",
        "    Output('input_transcript', 'style'),\n",
        "    Output('output_transcript', 'style'),\n",
        "    Output('clientside-store-data', 'data'),\n",
        "    Output('input_yt', 'value'),\n",
        "    Output('input_transcript', 'value'),\n",
        "     Output(\"loading-output\", \"children\")],\n",
        "    [Input('btn_submit', 'n_clicks'),\n",
        "    Input('btn_reset', 'n_clicks')],\n",
        "    [State('input_yt', 'value'),\n",
        "    State('input_transcript', 'value')]\n",
        ")\n",
        "def embed_video(btn_submit_n_clicks, btn_reset_n_clicks, link, transcript):\n",
        "    global submit_clicks\n",
        "    global reset_clicks\n",
        "\n",
        "    initial_message_style = {'display': 'block'}\n",
        "    invalid_style = {'color': 'red', 'display': 'none'}\n",
        "    no_transcript = {'color': 'red', 'display': 'none'}\n",
        "\n",
        "    input_style = {'width': '29%', 'height': '80%', 'display': 'inline-block', 'overflow-y': 'auto', 'border-style': 'solid', 'border-width': '1px'}\n",
        "    output_style = {'width': '29%', 'height': '80%', 'display': 'none', 'overflow-y': 'auto', 'border-style': 'solid', 'border-width': '1px'}\n",
        "\n",
        "    dict_words = {}\n",
        "    url = \"\"\n",
        "    if btn_submit_n_clicks > submit_clicks:\n",
        "        submit_clicks += 1\n",
        "        # TODO maybe add loading bar\n",
        "        video_id = extract_video_id(link)\n",
        "        if not video_id or not transcript:\n",
        "            initial_message_style = {'display': 'none'}\n",
        "            if not video_id:\n",
        "                invalid_style = {'color': 'red', 'display': 'block'}\n",
        "            if not transcript:\n",
        "                no_transcript = {'color': 'red', 'display': 'block'}\n",
        "        else:\n",
        "            url = link\n",
        "            input_style, output_style = output_style, input_style\n",
        "            waveform, sr = get_wav_sr_from_yt_video_id(video_id)\n",
        "            _, _, trellis, _, _, word_segments = execute(waveform, transcript)\n",
        "            dict_words = pd.DataFrame([vars(f) for f in word_segments]).to_dict('records')\n",
        "    \n",
        "    if btn_reset_n_clicks > reset_clicks:\n",
        "        reset_clicks += 1\n",
        "        transcript = \"\"\n",
        "\n",
        "    return url, initial_message_style, invalid_style, no_transcript, input_style, output_style, dict_words, url, transcript, None\n",
        "\n",
        "app.clientside_callback(\n",
        "    \"\"\"\n",
        "    function highlightWords(n_intervals, current_time, input, data) {\n",
        "        let pre_transcript = \"\";\n",
        "        let pre_highlight_word = \"\";\n",
        "        let highlight_word = \"\";\n",
        "        let post_highlight_word = \"\";\n",
        "        let post_transcript = \"\";\n",
        "        var listLength = data.length;\n",
        "        for (var i = 0; i < listLength; i++) {\n",
        "            let word = data[i];\n",
        "            if (current_time != null) {\n",
        "                start = word['start'];\n",
        "                end = word['end'];\n",
        "                if (current_time <= start - 1){\n",
        "                    post_transcript += ' ' + word['label'];\n",
        "                }\n",
        "                if (current_time >= start - 1 && current_time <= start){\n",
        "                    post_highlight_word += ' ' + word['label'];\n",
        "                }\n",
        "                if (current_time >= start && current_time <= end){\n",
        "                    highlight_word += word['label'];\n",
        "                }\n",
        "                if (current_time >= end && current_time <= end + 1){\n",
        "                    pre_highlight_word += word['label'] + ' ';\n",
        "                }\n",
        "                if (current_time >= end +1){\n",
        "                    pre_transcript += word['label'] + ' ';\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        return [pre_transcript, pre_highlight_word, highlight_word, post_highlight_word, post_transcript];\n",
        "    }\n",
        "    \"\"\",\n",
        "    [Output('pre_transcript', 'children'),\n",
        "    Output('pre_highlight_word', 'children'),\n",
        "    Output('highlight_word', 'children'),\n",
        "    Output('post_highlight_word', 'children'),\n",
        "    Output('post_transcript', 'children')],\n",
        "    [Input('interval', 'n_intervals')],\n",
        "    [State('player', 'currentTime'),\n",
        "     State('input_transcript', 'value'),\n",
        "     State('clientside-store-data', 'data')]\n",
        ")\n",
        "\n",
        "@app.callback(\n",
        "    Output('genre_class', 'children'),\n",
        "    Input('btn_submit', 'n_clicks'),\n",
        "    State('input_transcript', 'value')\n",
        ")\n",
        "def display_genre(n_clicks, transcript):\n",
        "    return predict_genre(transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTqlZniJ3IJx"
      },
      "outputs": [],
      "source": [
        "# click link to open website in new tab\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(mode='inline')\n",
        "    # app.run_server(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}